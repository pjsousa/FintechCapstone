{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from utils.datafetch import *\n",
    "from utils.vectorized_funs import *\n",
    "from utils.datapipe import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll need some way of loading our possible tickets.\n",
    "\n",
    "> We'll have some information for AMEX, NASDAQ and NYSE on one csv each in the config folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr = load_exchangesinfos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> We'll look into the duplicate tickers when applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_r = duplicate_tickers(rr, ign_exchange=True)\n",
    "_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in this case the ticker is the same company appearing in two exchanges, so we'll drop one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr = rr.drop([2922])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this time this should be empty...\n",
    "\n",
    "duplicate_tickers(rr, ign_exchange=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lets have a look at your datareader wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fetch_quotes(\"AMZN\", from_date=datetime.datetime(2016,12,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets see how we could prepare an initial data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tkrs = dict()\n",
    "errs = dict()\n",
    "itr = None\n",
    "\n",
    "for tkr in rr.iloc[0:5][\"Symbol\"].values.tolist():\n",
    "    itr = fetch_quotes(tkr)\n",
    "    if itr is None:\n",
    "        errs[tkr] = itr\n",
    "        print(\"ERROR for {}\".format(tkr))\n",
    "    else:\n",
    "        tkrs[tkr] = itr\n",
    "        print(\"Received {}\".format(tkr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_r = initial_dataload([\"AAPL\", \"TWTR\", \"XXII\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lets try with the ticker dataframe now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ticker_list = rr.iloc[0:100][\"Symbol\"].values\n",
    "#ticker_list = rr[\"Symbol\"].values\n",
    "_r = initial_dataload(ticker_list, verbose=True, del_temp=True)\n",
    "ticker_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ It is nicer to have the time from start to finish of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Let's try to read back in one of the files we just created...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XXII = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"XXII\"))\n",
    "\n",
    "print(\"Shape: {}\".format(XXII.shape))\n",
    "print(\"\\nStats\\n\")\n",
    "print(XXII.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets see what happens if the file does not exist....\n",
    "> We might want to consider the FileNotFoundError..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    XXIIAAAAAAAAA = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"XXIIAAAAAAAAA\"))\n",
    "except (FileNotFoundError):\n",
    "    print(\"File not found.\")\n",
    "    print(\"We could create a function to download the data instead of erroring...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> While we're here, let's look at the diference between csv and hdfs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_tkrs = [\"AAPL\", \"TWTR\", \"XXII\", \"AMZN\", \"GOOG\"]\n",
    "\n",
    "_r = initial_dataload(_tkrs)\n",
    "\n",
    "for t in _tkrs:\n",
    "    itr = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, t))\n",
    "    itr.to_hdf(\"{}/{}.h5\".format(DATA_PATH, t), \"stock_daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data\n",
    "pwd\n",
    "ls -lh\n",
    "du -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ok, so hdf5 will help us manage really large files. But is a bit of overkill for this kind of problem right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_stock = dict()\n",
    "\n",
    "_stock[\"AAPL\"] = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"AAPL\"))\n",
    "_stock[\"TWTR\"] = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"TWTR\"))\n",
    "_stock[\"XXII\"] = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"XXII\"))\n",
    "_stock[\"GOOG\"] = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"GOOG\"))\n",
    "_stock[\"AMZN\"] = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"AMZN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, lets settle on a little ticker list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "_tkrs_df = pd.DataFrame(_tkrs, columns=[\"ticker\"])\n",
    "_tkrs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us take a look at their min and max dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_tkrs_df[\"min\"] = _tkrs_df[\"ticker\"].apply(lambda s: _stock[s][\"Date\"].min())\n",
    "_tkrs_df[\"max\"] = _tkrs_df[\"ticker\"].apply(lambda s: _stock[s][\"Date\"].max())\n",
    "\n",
    "_tkrs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have been ignoring the fact that the dates come as strings from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_stock[\"GOOG\"][\"Date\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the **dtype** is \"object\" instead of \"date\".\n",
    "\n",
    "> We can take care of this using **to_datetime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(_stock[\"GOOG\"][\"Date\"], infer_datetime_format=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_dates(s):\n",
    "    _stock[s][\"Date\"] = pd.to_datetime(_stock[s][\"Date\"], infer_datetime_format=True)\n",
    "\n",
    "_a = _tkrs_df[\"ticker\"].apply(convert_dates)\n",
    "\n",
    "print(_stock[\"GOOG\"][\"Date\"].head())\n",
    "print(\"\\n\")\n",
    "print(_stock[\"AAPL\"][\"Date\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_stock[\"GOOG\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_tkrs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From now on it is safe to slice the *Date* columns using datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_stock[\"GOOG\"][\"Date\"].head() > datetime.datetime(2004, 8, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_stock[\"GOOG\"][\"Date\"].head() > datetime.datetime(2004, 8, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets take a look at rolling indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.rolling_mean(_stock[\"GOOG\"][\"Open\"], 5).iloc[0:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_stock[\"GOOG\"][\"Open\"].rolling(window=5, center=False).mean().iloc[0:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_stock[\"GOOG\"][\"Close\"].rolling(window=5, center=False).mean().iloc[0:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One can slice a subset of columns using loc and then call apply across all the sliced columns.\n",
    "\n",
    "> The result will be a new dataframe with the rolled averages row-by-row across each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_rolling_mean(s):\n",
    "    return s.rolling(window=5, center=False).mean()\n",
    "\n",
    "_rolling5 = _stock[\"GOOG\"].loc[:, [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"]].apply(apply_rolling_mean)\n",
    "_rolling5.iloc[0:14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can concatenate the new frame and the old one using concat (we need to set the *axis=* in order to do it across columns)\n",
    "\n",
    "> We can also reset the column names so that we don't have duplicate names (we could have done this in the previous step on the _rolling5 frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_result = pd.concat([_stock[\"GOOG\"], _rolling5], axis=1)\n",
    "_result.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close', 'Open_roll5','High_roll5', 'Low_roll5', 'Close_roll5', 'Volume_roll5', 'Adj Close_roll5']\n",
    "\n",
    "_result.iloc[0:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Lets see how we could repurpose **apply_rolling_mean** to use with different window sizes with an apply over columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_rolling_mean(s, window=5):\n",
    "    return s.rolling(window=window, center=False).mean()\n",
    "\n",
    "_col_slice = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"]\n",
    "_pre_bound = partial(apply_rolling_mean, window=3)\n",
    "_rolling = _stock[\"GOOG\"].loc[ : , _col_slice ].apply(_pre_bound)\n",
    "_rolling.iloc[0:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also call rolling directly from the dataframe itself\n",
    "_stock[\"GOOG\"].loc[:,[\"Open\"]].rolling(window=3, center=False).mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this will be the same as the cells above\n",
    "a = _stock[\"GOOG\"].loc[:,[\"Open\"]].rolling(window=3, center=False)\n",
    "getattr(a, 'mean')().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# but now we can call other rolling stats programatically\n",
    "\n",
    "stat_fns = [\"mean\", \"sum\"]\n",
    "\n",
    "for itr in stat_fns:\n",
    "    print(\"{} :\\n\".format(itr))\n",
    "    print(getattr(a, itr)().head())\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can bundle all this rolling logic in a function that rolls and merges the collumns that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roll_columns(_stock[\"GOOG\"], \"mean\", column_slice=[\"Open\"]).iloc[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roll_columns(_stock[\"GOOG\"], \"mean\", column_slice=[\"Open\", \"Close\"]).iloc[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roll_columns(_stock[\"GOOG\"], \"mean\", column_slice=[\"Open\", \"Close\"], window=5).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "roll_columns(_stock[\"GOOG\"], \"mean\", column_slice=[\"Open\", \"Close\"], window=3).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roll_columns(_stock[\"GOOG\"], \"mean\", column_slice=[\"Open\", \"Close\"], window=3, merge_result=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We really shouldn't leave the nan or other missing values there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_topad = roll_columns(_stock[\"GOOG\"], \"mean\", column_slice=[\"Open\", \"Close\"], window=3).head()\n",
    "df_topad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topad.fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we could acomodate this on our function...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roll_columns(_stock[\"GOOG\"], \"mean\", column_slice=[\"Open\", \"Close\"], window=3, pad_result=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roll_columns(_stock[\"GOOG\"], \"mean\", column_slice=[\"Open\", \"Close\"], window=3, pad_result=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can look at how to compute the daily returns by shifting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_sample_goog_close = _stock[\"GOOG\"].loc[:,[\"Close\"]].iloc[:15]\n",
    "_sample_goog_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "_rr = (_sample_goog_close[1:] / _sample_goog_close[:-1].values) - 1\n",
    "_rr.ix[0] = 0\n",
    "_rr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# this code does the same as the cell above, but as we can see it is much faster\n",
    "_rr = (_sample_goog_close / _sample_goog_close.shift(1)) - 1\n",
    "_rr.ix[0] = 0\n",
    "_rr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets try with multiple collumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "_rr = (_sample_goog_close / _sample_goog_close.shift(1)) - 1\n",
    "_rr.ix[0] = 0\n",
    "_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_rr = (_sample_goog_close / _sample_goog_close.shift(1)) - 1\n",
    "_rr = _rr.fillna(0)\n",
    "_rr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets try out some functions of our own. Built on top of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame([[1, 1.2, 1.8],[1, 0.8, 1.6],[1,-0.5,0.1]]).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timewindow_diff(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timewindow_diff(df, shift_window=2, fillna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timewindow_diff(df, column_slice=[1], fillna=True, merge_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timewindow_return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> timewindow_return gives us the variation from an initial value. See the explanation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 200\n",
    "p = 1.5\n",
    "v = p * n\n",
    "print(\"You buy {} shares at {}. Your value is {}\".format(n,p,v))\n",
    "np = 1.0\n",
    "nv = np * n\n",
    "r = ((np / p) - 1.0)\n",
    "print(\"The price changes to {}. You new value is {}.\".format(np, nv))\n",
    "\n",
    "print (\"Your return is {} ({} of your initial value)\".format(v*r, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timewindow_cumdiff(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timewindow_cumreturn(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, lets take a real look at dates and how to manage them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This can create an array with a date for us\n",
    "ledate = np.array('2015-12-30', dtype=np.datetime64)\n",
    "ledate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we can obviously do vectorized operation on *ledate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lerange_12day = ledate + np.arange(12)\n",
    "lerange_12day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also use *np.datetime64* in order to create a date constant. It's possible to give it a time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.datetime64('2015-07-04 12:00'))\n",
    "print(np.datetime64('2015-07-04 12:59:59.50', 'ns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These are the code formats possible with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Code    | Meaning     | Time span (relative) | Time span (absolute)   |\n",
    "|--------|-------------|----------------------|------------------------|\n",
    "| ``Y``  | Year\t       | ± 9.2e18 years       | [9.2e18 BC, 9.2e18 AD] |\n",
    "| ``M``  | Month       | ± 7.6e17 years       | [7.6e17 BC, 7.6e17 AD] |\n",
    "| ``W``  | Week\t       | ± 1.7e17 years       | [1.7e17 BC, 1.7e17 AD] |\n",
    "| ``D``  | Day         | ± 2.5e16 years       | [2.5e16 BC, 2.5e16 AD] |\n",
    "| ``h``  | Hour        | ± 1.0e15 years       | [1.0e15 BC, 1.0e15 AD] |\n",
    "| ``m``  | Minute      | ± 1.7e13 years       | [1.7e13 BC, 1.7e13 AD] |\n",
    "| ``s``  | Second      | ± 2.9e12 years       | [ 2.9e9 BC, 2.9e9 AD]  |\n",
    "| ``ms`` | Millisecond | ± 2.9e9 years        | [ 2.9e6 BC, 2.9e6 AD]  |\n",
    "| ``us`` | Microsecond | ± 2.9e6 years        | [290301 BC, 294241 AD] |\n",
    "| ``ns`` | Nanosecond  | ± 292 years          | [ 1678 AD, 2262 AD]    |\n",
    "| ``ps`` | Picosecond  | ± 106 days           | [ 1969 AD, 1970 AD]    |\n",
    "| ``fs`` | Femtosecond | ± 2.6 hours          | [ 1969 AD, 1970 AD]    |\n",
    "| ``as`` | Attosecond  | ± 9.2 seconds        | [ 1969 AD, 1970 AD]    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also use pandas to manipulate our dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ledate = pd.to_datetime(\"4th of July, 2015\")\n",
    "ledate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ledate.strftime('%A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also use vectorized operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lerange_12days = ledate + pd.to_timedelta(np.arange(12), 'D')\n",
    "lerange_12days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lerange_12months = ledate + pd.to_timedelta(np.arange(12), 'M')\n",
    "lerange_12months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lerange_12minutes = ledate + pd.to_timedelta(np.arange(12), 'm')\n",
    "lerange_12minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also use *pd.date_range* to achieve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', '2015-07-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', periods=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.date_range('2015-07-03', periods=8, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.period_range('2015-07', periods=8, freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequencies and Offsets\n",
    "(Taken from [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook))\n",
    "\n",
    "Fundamental to these Pandas time series tools is the concept of a frequency or date offset.\n",
    "Just as we saw the ``D`` (day) and ``H`` (hour) codes above, we can use such codes to specify any desired frequency spacing.\n",
    "The following table summarizes the main codes available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Code   | Description         | Code   | Description          |\n",
    "|--------|---------------------|--------|----------------------|\n",
    "| ``D``  | Calendar day        | ``B``  | Business day         |\n",
    "| ``W``  | Weekly              |        |                      |\n",
    "| ``M``  | Month end           | ``BM`` | Business month end   |\n",
    "| ``Q``  | Quarter end         | ``BQ`` | Business quarter end |\n",
    "| ``A``  | Year end            | ``BA`` | Business year end    |\n",
    "| ``H``  | Hours               | ``BH`` | Business hours       |\n",
    "| ``T``  | Minutes             |        |                      |\n",
    "| ``S``  | Seconds             |        |                      |\n",
    "| ``L``  | Milliseonds         |        |                      |\n",
    "| ``U``  | Microseconds        |        |                      |\n",
    "| ``N``  | nanoseconds         |        |                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Code    | Description            || Code    | Description            |\n",
    "|---------|------------------------||---------|------------------------|\n",
    "| ``MS``  | Month start            ||``BMS``  | Business month start   |\n",
    "| ``QS``  | Quarter start          ||``BQS``  | Business quarter start |\n",
    "| ``AS``  | Year start             ||``BAS``  | Business year start    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This will give us 9 spans of 2 and a half hours each\n",
    "pd.timedelta_range(0, periods=9, freq=\"2H30T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl = _stock[\"AAPL\"]\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goog = _stock[\"GOOG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_rolling = aapl.loc[:100, [\"Close\"]].rolling(window=10, center=True)\n",
    "\n",
    "_rolling.corr(other=goog[\"Open\"]).iloc[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl[\"Close\"].corr(goog[\"Close\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets take a look at the exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_tkrs = [\"^IXIC\", \"^GSPC\"]\n",
    "\n",
    "_r = initial_dataload(_tkrs)\n",
    "\n",
    "_exchanges = dict()\n",
    "\n",
    "for t in _tkrs:\n",
    "    _exchanges[t] = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_beta = pd.DataFrame()\n",
    "\n",
    "df_beta[\"IXIC\"] = _exchanges[\"^IXIC\"].iloc[-100:, :][\"Close\"]\n",
    "df_beta[\"GOOG\"] = _stock[\"GOOG\"].iloc[-100:, :][\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_beta(df):\n",
    "\t# first column is the market\n",
    "\tX = df.values[:, [0]]\n",
    "\t# prepend a column of ones for the intercept\n",
    "\tX = np.concatenate([np.ones_like(X), X], axis=1)\n",
    "\t# matrix algebra\n",
    "\tb = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(df.values[:, 1:])\n",
    "\treturn pd.Series(b[1], df.columns[1:], name='Beta')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_beta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_stock[\"GOOG\"].iloc[-100:, :][\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = pd.date_range('1995-12-31', periods=480, freq='M', name='Date')\n",
    "stoks = pd.Index(['s{:04d}'.format(i) for i in range(4000)])\n",
    "df = pd.DataFrame(np.random.rand(480, 4000), dates, stoks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.iloc[:5, :5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def roll(df, w):\n",
    "    # stack df.values w-times shifted once at each stack\n",
    "    roll_array = np.dstack([df.values[i:i+w, :] for i in range(len(df.index) - w + 1)]).T\n",
    "    # roll_array is now a 3-D array and can be read into\n",
    "    # a pandas panel object\n",
    "    panel = pd.Panel(roll_array, \n",
    "                     items=df.index[w-1:],\n",
    "                     major_axis=df.columns,\n",
    "                     minor_axis=pd.Index(range(w), name='roll'))\n",
    "    # convert to dataframe and pivot + groupby\n",
    "    # is now ready for any action normally performed\n",
    "    # on a groupby object\n",
    "    return panel.to_frame().unstack().T.groupby(level=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beta(df):\n",
    "    # first column is the market\n",
    "    X = df.values[:, [0]]\n",
    "    # prepend a column of ones for the intercept\n",
    "    X = np.concatenate([np.ones_like(X), X], axis=1)\n",
    "    # matrix algebra\n",
    "    b = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(df.values[:, 1:])\n",
    "    return pd.Series(b[1], df.columns[1:], name='Beta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdf = roll(df, 12)\n",
    "betas = rdf.apply(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_beta(df):\n",
    "    np_array = df.values\n",
    "    m = np_array[:,0] # market returns are column zero from numpy array\n",
    "    s = np_array[:,1] # stock returns are column one from numpy array\n",
    "    covariance = np.cov(s,m) # Calculate covariance between stock and market\n",
    "    beta = covariance[0,1]/covariance[1,1]\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(calc_beta(df.iloc[:12, :2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(beta(df.iloc[:12, :2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas = rdf.apply(beta)\n",
    "betas.iloc[:5, :5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "roll(df, 12).apply(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "x = np.random.random(10)\n",
    "y = np.random.random(10)\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_stock = dict()\n",
    "_stock[\"TWTR\"] = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"TWTR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timespan = None\n",
    "\n",
    "timespan = {\n",
    "    \"short_term\": [1,5,10]\n",
    "    ,\"medium_term\": [30,50,70]\n",
    "    ,\"long_term\": [100, 200, 400]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gains = None\n",
    "\n",
    "gains = {\n",
    "    \"low\": [0.01]\n",
    "    ,\"medium\": [0.05, 0.07, 0.1]\n",
    "    ,\"high\": [0.2, 0.4, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = None\n",
    "\n",
    "losses = {\n",
    "    \"low\": [-0.05, -0.1]\n",
    "    ,\"medium\": [-0.3, -0.5]\n",
    "    ,\"high\": [-0.8, -1.0, -2.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itr_df = _stock[\"TWTR\"];\n",
    "\n",
    "itr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## calculate diff moves for timespans\n",
    "## calculate returns for timespans\n",
    "## calculate moving averages\n",
    "## calculate bollinger bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## calculate diff moves for timespans\n",
    "\n",
    "itr_df = _stock[\"TWTR\"];\n",
    "\n",
    "for ts_name in timespan:\n",
    "    for t in timespan[ts_name]:\n",
    "        itr_df = timewindow_diff(itr_df, column_slice=[\"Close\", \"Volume\"], shift_window=t, merge_result=True)        \n",
    "\n",
    "itr_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## calculate returns for timespans\n",
    "\n",
    "itr_df = _stock[\"TWTR\"];\n",
    "\n",
    "for ts_name in timespan:\n",
    "    for t in timespan[ts_name]:\n",
    "        itr_df = timewindow_return(itr_df, column_slice=[\"Close\", \"Volume\"], shift_window=t, merge_result=True)        \n",
    "\n",
    "itr_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## calculate moving averages\n",
    "\n",
    "itr_df = _stock[\"TWTR\"];\n",
    "\n",
    "for ts_name in timespan:\n",
    "    for t in timespan[ts_name]:\n",
    "        itr_df = roll_columns(itr_df, \"mean\", column_slice=[\"Close\", \"Volume\"], window=t, merge_result=True)        \n",
    "\n",
    "itr_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## calculate bollinger bands\n",
    "\n",
    "itr_df = pd.DataFrame()\n",
    "res_df = pd.DataFrame()\n",
    "\n",
    "for ts_name in timespan:\n",
    "    if ts_name in [\"medium_term\", \"long_term\"]:\n",
    "        for t in timespan[ts_name]:\n",
    "            itr_df = roll_columns(_stock[\"TWTR\"], \"std\", column_slice=[\"Close\"], window=t, merge_result=False, scaler=2, pad_result=True)\n",
    "            upper_band = itr_df.apply(lambda x: s + x)\n",
    "            lower_band = itr_df.apply(lambda x: s - x)\n",
    "            \n",
    "            roll_name = \"{}_roll_2std_{}\".format(\"Close\", t)\n",
    "            upper_name = \"{}_bollinger_{}_up\".format(\"Close\", t)\n",
    "            lower_name = \"{}_bollinger_{}_low\".format(\"Close\", t)\n",
    "            \n",
    "            res_df[roll_name] = itr_df.iloc[:, 0]\n",
    "            res_df[upper_name] = upper_band\n",
    "            res_df[lower_name] = lower_band\n",
    "\n",
    "            \n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_stock = dict()\n",
    "_stock[\"TWTR\"] = pd.read_csv(\"{}/{}.csv\".format(DATA_PATH, \"TWTR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl = load_raw_frame(\"AAPL\")\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timespan = None\n",
    "\n",
    "timespan = {\n",
    "    \"medium_term\": [4,7]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aapl = load_raw_frame(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calc_diff_moves(aapl, timespan=timespan, column_slice=[\"Close\", \"Volume\"], fillna=True, merge_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "calc_return(aapl, timespan=timespan, column_slice=[\"Close\", \"Volume\"], merge_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_sma(aapl, timespan, [\"Close\", \"Volume\"], merge_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_bollinger(aapl, timespan, [\"Close\", \"Volume\"], merge_result=True, scaler=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp500 = load_raw_frame(\"^GSPC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp500_i = sp500.set_index(\"Date\")\n",
    "aapl_i = aapl.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_df = pd.merge(aapl_i.loc[:,[\"Close\"]], sp500_i.loc[:,[\"Close\"]], how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timewindow_beta(aapl, sp500, [\"Close\"], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"1980-12-26     0.602909\n",
    "1980-12-29     0.462633\n",
    "1980-12-30     0.362929\n",
    "1980-12-31     0.272490\n",
    "1981-01-02     0.251707\n",
    "1981-01-05     0.193142\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_i = 10\n",
    "\n",
    "s = joined_df.iloc[ 0:10, :][\"Close_x\"]\n",
    "m = joined_df.iloc[ 0:10, :][\"Close_y\"]\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(s.values,m.values)\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timewindow_alpha(aapl, sp500, [\"Close\",\"Volume\",\"Low\"], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
